
The goal of this data analysis is to predict the manner in which the user did the exercise. The training data set contains 
the target variable classe, all other variables will be used to predict for it. Using cross validation, I will find the 
model that best fits the data. I started by cleaning the dataset, removing columns that were not related to accelerometer 
reading and readings that were dominated by NA values. This reduced the variables from 160 to 53, a more manageable amount. 
I started with a fast recursive partitioning model to start to see if that would produce reasonable predictions. Unfortunately
the estimated out of sample error for the rpart model was 51% and far to high. Next I tried a random forest model with 3-fold 
cross validation. This model performed really well with an estimated out of sample of only 0.7%. Using this model, 
20 predictions will be made for the test data set.

Load Dedepdencies

library(caret)
library(rpart)
library(rattle)
library(scales)
library(randomForest)
set.seed(1337)
Download and Load Data

train_data_url = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
test_data_url = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'

if (file.exists('data/pml-training.csv') == FALSE) {
  download.file(train_data_url, 'data/pml-training.csv')
}
if (file.exists('data/pml-testing.csv') == FALSE) {
  download.file(test_data_url, 'data/pml-testing.csv')
}

pmlTrainingData <- read.csv('data/pml-training.csv', na.strings=c("","NA"))
finalTest <- read.csv('data/pml-testing.csv', na.strings=c("","NA"))
Create Training & Cross Validation Datasets

The full training dataset it split into a training dataset and a testing dataset. The testing data will be used to cross validate our models.

inTrain <- createDataPartition(pmlTrainingData$classe, p=.7, list=FALSE)
training <- pmlTrainingData[inTrain,]
testing <- pmlTrainingData[-inTrain,]

summary(training$classe)
Clean Data

Next, time-related & recording variables and the row index variable X are removed because the purpose of the machine learning assignment is to use accelerometer reads to make predictions.

training <- training[, -c(1:7)]
testing <- testing[, -c(1:7)]
finalTest <- finalTest[, -c(1:7)]
First, I removed variables which contained a majority of missing values. NAs and blank fields were both marked as NA when the CSV was read.

mostlyNAs <- which(colSums(is.na(training)) > nrow(training)/2)
training <- training[, -mostlyNAs]
testing <- testing[, -mostlyNAs]
finalTest <- finalTest[, -mostlyNAs]
Machine Learning

Recursive partitioning Model

Starting with a simple model Train the decision tree model

rpModelFit <- train(classe ~ ., method="rpart", data=training)
rpModelFit$finalModel
Plot the model

fancyRpartPlot(rpModelFit$finalModel, sub='')
Predict classe for cross validation dataset

rpPreds <- predict(rpModelFit, newdata=testing)
rpConMatrix <- confusionMatrix(rpPreds, testing$classe)
rpConMatrix
Low accuracy with Recursive partitioning model

rpAccuracy = rpConMatrix$overall[[1]]
percent(rpAccuracy)
The estimated out of sample error with the cross validation dataset for this model is

percent(1.00-rpAccuracy)
Random Forest Model

Random Forests should be a better learning model for our dataset.

fitControl <- trainControl(method="cv", number=3, verboseIter=F)
rfModelFit <- train(classe ~., method="rf", data=training, trControl=fitControl)
rfModelFit$finalModel
Predict classe for cross validation dataset

rfPreds <- predict(rfModelFit, newdata=testing)
rfConMatrix <- confusionMatrix(rfPreds, testing$classe)
rfConMatrix
Much higher accuracy with a Random Forest Model

rfAccuracy = rfConMatrix$overall[[1]]
percent(rfAccuracy)
The estimated out of sample error with the cross validation dataset for this model is

percent(1.00-rfAccuracy)
Conclusion

The Random Forest model outperformed the the Recursive partitioning model by quite a bit. The random forest model was selected for final submissions of the project.

submissionPreds <- predict(rfModelFit, newdata=finalTest)
submissionPreds
